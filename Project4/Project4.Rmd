---
title: "DATA 607 - Project 4"
author: "Zach Alexander"
date: "October 28, 2019"
output: html_document
---

***
#### Libraries used
```{r setup, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(stringr)
library(SnowballC)
library(tidyverse)
library(stringi)
library(corpus)
library(wordcloud)
library(e1071)
library(NLP)
library(kableExtra)
library(corpus)
library(caret)
library(randomForest)
```

***

#### Downloading the emails

```{r, message=FALSE, warning=FALSE}
download.file(url = "http://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2", destfile = "20021010_easy_ham.tar.bz2")
untar("20021010_easy_ham.tar.bz2",compressed = "bzip2")
ham_emails = list.files(path = "easy_ham",full.names = TRUE)

download.file(url = "http://spamassassin.apache.org/old/publiccorpus/20030228_spam_2.tar.bz2", destfile = "20030228_spam_2.tar.bz2")
untar("20030228_spam_2.tar.bz2", compressed = "bzip2")
spam_emails = list.files(path = "spam_2", full.names = TRUE)
```

***

#### Saving all the emails in a "spam" and "ham" list
```{r}
spam_emails_list <- NA
for (i in 1:length(spam_emails)){
  email_text1 <- readLines(spam_emails[i])
  email_list1 <- list(paste(email_text1, collapse = "\n"))
  spam_emails_list <- c(spam_emails_list, email_list1)
}

ham_emails_list <- NA
for (i in 1:length(ham_emails)){
  email_text <- readLines(ham_emails[i])
  email_list <- list(paste(email_text, collapse = "\n"))
  ham_emails_list <- c(ham_emails_list, email_list)
}

# removing blank first item in lists
ham_emails_list <- ham_emails_list[-1]
spam_emails_list <- spam_emails_list[-1]
```

***
#### Creating dataframes from "spam" and "ham" lists

```{r}
ham_df <- data.frame(unlist(ham_emails_list))
ham_df$email_group <- 'ham'

ham_df <- ham_df %>%
  rename("email_text" = unlist.ham_emails_list.)

spam_df <- data.frame(unlist(spam_emails_list))
spam_df$email_group <- 'spam'

spam_df <- spam_df %>%
  rename("email_text" = unlist.spam_emails_list.)
```

***
#### Combining the two dataframes into one
```{r}
full_email_df <- rbind(ham_df, spam_df)
full_email_df$email_group <- factor(full_email_df$email_group)

set.seed(3453)

full_email_df <- full_email_df[sample(nrow(full_email_df)),]
spam_idx_v <- which(full_email_df$email_group == "spam")
ham_idx_v <- which(full_email_df$email_group == "ham")
```

***
#### Fixing encoding of email text
```{r}
rm(email_list1, email_list, i, email_text, email_text1, ham_emails_list, spam_emails_list)
# spam_df$email_text <- iconv(spam_df$email_text, "ASCII", "UTF-8", sub="byte")
# ham_df$email_text <- iconv(ham_df$email_text, "ASCII", "UTF-8", sub="byte")
# full_email_df$email_text <- iconv(full_email_df$email_text, "ASCII", "UTF-8", sub="byte")
```


***

#### Creating a corpus and cleaning the corpus
```{r, warning=FALSE, message=FALSE}
full_email_corpus <- Corpus(VectorSource(full_email_df$email_text))

full_email_corpus_cleaned <- full_email_corpus %>%
    tm_map(stripWhitespace) %>%
    tm_map(content_transformer(tolower)) %>%
    tm_map(removePunctuation) %>%
    tm_map(removeWords, stopwords()) %>% 
    tm_map(removeNumbers)
```

***
#### Creating term document matrix
```{r}
full_email_dtm <- DocumentTermMatrix(full_email_corpus_cleaned)
```

***
#### Generate word clouds
```{r}
# spam word cloud
suppressWarnings(wordcloud(full_email_corpus_cleaned[spam_idx_v], min.freq=550))

# ham word cloud
suppressWarnings(wordcloud(full_email_corpus_cleaned[ham_idx_v], min.freq=550))
```

***

#### Looking at sparse terms
```{r}
# remove terms that do not comprise of at least 75 percent of documents
full_emaildtm_sparse <- removeSparseTerms(full_email_dtm, 0.75)
full_emaildtm_sparse
```
***

#### Preparing the data for random forest
```{r}
# split the randomized data frame into a 70/30 split
train_index <- createDataPartition(full_email_df$email_group, p = 0.70, list = FALSE)

# 70 percent goes to the model_train
model_train <- full_email_df[train_index,]

# 30 percent goes to the model_test
model_test <- full_email_df[-train_index,]

# create document term matrices
model_dtm_train <- full_email_dtm[train_index,]
model_dtm_test <- full_email_dtm[-train_index,]

# create corpuses
model_corpus_train<- full_email_corpus_cleaned[train_index]
model_corpus_test<- full_email_corpus_cleaned[-train_index]

# make a data frame from the document term matrices for random forest and add a column with the factor
training_set <- as.data.frame(as.matrix(model_dtm_train))
training_set$email_group <- model_train$email_group

test_set <- as.data.frame(as.matrix(model_dtm_test))
test_set$email_group <- model_test$email_group
```

***

#### Our data is ready for random forest (full dataset)
```{r}
# create the classifier and run random forest
random_forest_classifier <- randomForest(x = training_set[-97788], y = training_set$email_group, ntree = 10)

# use the classifier to predict spam/ham on the test dataset
prediction <- predict(random_forest_classifier, newdata = test_set[-97788])

# look at the results
confusion_matrix <- table(test_set[, 97788], prediction)
confusion_matrix
```

***

#### Using sparse dataset with random forest (cut down on processing time)

```{r}
# create document term matrices
model_sparsedtm_train <- full_emaildtm_sparse[train_index,]
model_sparsedtm_test <- full_emaildtm_sparse[-train_index,]


# make a data frame from the document term matrices for random forest and add a column with the factor
training_sparse_set <- as.data.frame(as.matrix(model_sparsedtm_train))
training_sparse_set$email_group <- model_train$email_group

test_sparse_set <- as.data.frame(as.matrix(model_sparsedtm_test))
test_sparse_set$email_group <- model_test$email_group

# create the classifier and run random forest
random_forest_sparse_classifier <- randomForest(x = training_sparse_set[-68], y = training_sparse_set$email_group, ntree = 10)

# use the classifier to predict spam/ham on the test dataset
prediction_sparse <- predict(random_forest_sparse_classifier, newdata = test_sparse_set[-68])

# look at the results
confusion_matrix_sparse <- table(test_sparse_set[, 68], prediction_sparse)
confusion_matrix_sparse
```
